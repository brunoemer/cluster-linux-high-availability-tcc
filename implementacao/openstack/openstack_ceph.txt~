node 1
openstack controller node
-identity(keystone)
-image(glance)
-network(neutron)
-dashboard(horizon)(interface)
+openstack compute+storage node

node 2
openstack compute+storage node
-block storage(cinder)
-compute(nova)
ceph
-OSD
-mon
-ceph-deploy


#############################
## OPENSTACK ##
#############################
http://docs.openstack.org/mitaka/install-guide-ubuntu/environment-packages.html

####### controller node

#network 1 publica 186.195.16.0/24 e 2 para gerencia x.x.x.x

## NTP
#controller node
apt-get install chrony

# compute nodes
apt-get install chrony
#editar /etc/chrony/chrony.conf
server controller iburst
service chrony restart

apt-get install software-properties-common
add-apt-repository cloud-archive:mitaka
apt-get update
#client
apt-get install python-openstackclient

#mysql - configurar

#message queue
apt-get install rabbitmq-server
rabbitmqctl add_user openstack openstack
#senha openstack
rabbitmqctl set_permissions openstack ".*" ".*" ".*"

#memcached
apt-get install memcached python-memcache
#editar /etc/memcached.conf
-l 192.168.2.8
service memcached restart


##identify service
mysql -u root -p
CREATE DATABASE keystone;
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' \
  IDENTIFIED BY 'keystone';
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \
  IDENTIFIED BY 'keystone';

echo "manual" > /etc/init/keystone.override
apt-get install keystone apache2 libapache2-mod-wsgi

#configurar /etc/keystone/keystone.conf
admin_token=f5aa02fa1fa19a2975e4
[database]
connection = mysql+pymysql://keystone:keystone@controller/keystone
[token]
provider = fernet

apt-get install python-mysqldb
su -s /bin/sh -c "keystone-manage db_sync" keystone
keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
service keystone restart
rm -f /var/lib/keystone/keystone.db

#configurar apache /etc/apache2/sites-available/wsgi-keystone.conf
Listen 5000
Listen 35357

<VirtualHost *:5000>
    WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP}
    WSGIProcessGroup keystone-public
    WSGIScriptAlias / /usr/bin/keystone-wsgi-public
    WSGIApplicationGroup %{GLOBAL}
    WSGIPassAuthorization On
    ErrorLogFormat "%{cu}t %M"
    ErrorLog /var/log/apache2/keystone.log
    CustomLog /var/log/apache2/keystone_access.log combined

    <Directory /usr/bin>
        Require all granted
    </Directory>
</VirtualHost>

<VirtualHost *:35357>
    WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP}
    WSGIProcessGroup keystone-admin
    WSGIScriptAlias / /usr/bin/keystone-wsgi-admin
    WSGIApplicationGroup %{GLOBAL}
    WSGIPassAuthorization On
    ErrorLogFormat "%{cu}t %M"
    ErrorLog /var/log/apache2/keystone.log
    CustomLog /var/log/apache2/keystone_access.log combined

    <Directory /usr/bin>
        Require all granted
    </Directory>
</VirtualHost>

a2ensite wsgi-keystone.conf

#service = ?; endpoint = url/tcp da api REST; domain = ?; project = ?, user, role = ?
export OS_TOKEN=f5aa02fa1fa19a2975e4
export OS_URL=http://controller:35357/v3
export OS_IDENTITY_API_VERSION=3
openstack service create \
  --name keystone --description "OpenStack Identity" identity
openstack endpoint create --region RegionOne \
  identity public http://controller:5000/v3
openstack endpoint create --region RegionOne \
  identity internal http://controller:5000/v3
openstack endpoint create --region RegionOne \
  identity admin http://controller:35357/v3
openstack domain create --description "Default Domain" default
openstack project create --domain default \
  --description "Admin Project" admin
openstack user create --domain default \
  --password-prompt admin
#senha
openstack role create admin
openstack role add --project admin --user admin admin
openstack project create --domain default \
  --description "Service Project" service
openstack project create --domain default \
  --description "Demo Project" demo
openstack user create --domain default \
  --password-prompt demo
#senha
openstack role create user
openstack role add --project demo --user demo user

#editar /etc/keystone/keystone-paste.ini e remover admin_token_auth
unset OS_TOKEN OS_URL
openstack --os-auth-url http://controller:35357/v3 \
  --os-project-domain-name default --os-user-domain-name default \
  --os-project-name admin --os-username admin token issue
resultado:
| Field      | Value                                                                                                                                                                                   | expires    | 2016-08-06T20:56:58.795312Z                                                                                                                                                             |
| id         | gAAAAABXpkEKmrLBHdezvDVL62FWVlPVU4dZdtdLrNk8hbE6LB041RHSttQlZEZ4gnjVW6VmxH4hpR_bcWYOcY-Xvur7n6r1fsNWpdzTWPMN1F8bHTNx62u1pg1NDND1nA3BygeRbUAwqrsqwOq2wjrR_jByX4CISi03t5wNGXXQmQ1jM9QwufE |
| project_id | 7bac570b1684469396c1d72acba11e83                                                                                                                                                        |
| user_id    | e52d076db3264ecd9d22d74f2a6c5d15            

openstack --os-auth-url http://controller:5000/v3 \
  --os-project-domain-name default --os-user-domain-name default \
  --os-project-name demo --os-username demo token issue

#configurar openrc.sh - http://docs.openstack.org/user-guide/common/cli-set-environment-variables-using-openstack-rc.html
#criar arquivo USER-openrc.sh
export OS_PROJECT_DOMAIN_NAME=default
export OS_USER_DOMAIN_NAME=default
export OS_PROJECT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=admin
export OS_AUTH_URL=http://controller:35357/v3
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2
#executar: . USER-openrc.sh
. admin-openrc.sh


##image
mysql -u root -p
CREATE DATABASE glance;
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' \
  IDENTIFIED BY 'glance';
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' \
  IDENTIFIED BY 'glance';

. admin-openrc.sh
openstack user create --domain default --password-prompt glance
#senha
openstack role add --project service --user glance admin
openstack service create --name glance \
  --description "OpenStack Image" image
openstack endpoint create --region RegionOne \
  image public http://controller:9292
openstack endpoint create --region RegionOne \
  image internal http://controller:9292
openstack endpoint create --region RegionOne \
  image admin http://controller:9292

apt-get install glance

#editar /etc/glance/glance-api.conf
[database]
connection = mysql+pymysql://glance:glance@controller/glance
[keystone_authtoken]
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = glance
password = glance
[paste_deploy]
flavor = keystone
[glance_store]
stores = file,http
default_store = file
filesystem_store_datadir = /var/lib/glance/images/

#editar /etc/glance/glance-registry.conf
[database]
connection = mysql+pymysql://glance:glance@controller/glance
[keystone_authtoken]
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = glance
password = glance
[paste_deploy]
flavor = keystone

su -s /bin/sh -c "glance-manage db_sync" glance
service glance-registry restart
service glance-api restart

#verify
openstack image list


##compute on controller node
mysql -u root -p
CREATE DATABASE nova_api;
CREATE DATABASE nova;
GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' \
  IDENTIFIED BY 'nova';
GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' \
  IDENTIFIED BY 'nova';
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' \
  IDENTIFIED BY 'nova';
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' \
  IDENTIFIED BY 'nova';

. admin-openrc.sh
openstack user create --domain default \
  --password-prompt nova
#senha
openstack role add --project service --user nova admin
openstack service create --name nova \
  --description "OpenStack Compute" compute
openstack endpoint create --region RegionOne \
  compute public http://controller:8774/v2.1/%\(tenant_id\)s
openstack endpoint create --region RegionOne \
  compute internal http://controller:8774/v2.1/%\(tenant_id\)s
openstack endpoint create --region RegionOne \
  compute admin http://controller:8774/v2.1/%\(tenant_id\)s

apt-get install nova-api nova-conductor nova-consoleauth \
  nova-novncproxy nova-scheduler

#editar /etc/nova/nova.conf
[DEFAULT]
enabled_apis = osapi_compute,metadata
[api_database]
connection = mysql+pymysql://nova:nova@controller/nova_api
[database]
connection = mysql+pymysql://nova:nova@controller/nova
[DEFAULT]
rpc_backend = rabbit
auth_strategy = keystone
[oslo_messaging_rabbit]
rabbit_host = controller
rabbit_userid = openstack
rabbit_password = openstack
[keystone_authtoken]
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = nova
[DEFAULT]
my_ip = 192.168.2.8
use_neutron = True
firewall_driver = nova.virt.firewall.NoopFirewallDriver
[vnc]
vncserver_listen = $my_ip
vncserver_proxyclient_address = $my_ip
[glance]
api_servers = http://controller:9292
[oslo_concurrency]
lock_path = /var/lib/nova/tmp
#remover logdir

su -s /bin/sh -c "nova-manage api_db sync" nova
su -s /bin/sh -c "nova-manage db sync" nova
service nova-api restart
service nova-consoleauth restart
service nova-scheduler restart
service nova-conductor restart
service nova-novncproxy restart

openstack compute service list


## network 
mysql -u root -p
CREATE DATABASE neutron;
GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' \
  IDENTIFIED BY 'neutron';
GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' \
  IDENTIFIED BY 'neutron';

. admin-openrc.sh
openstack user create --domain default --password-prompt neutron
#senha
openstack role add --project service --user neutron admin
openstack service create --name neutron \
  --description "OpenStack Networking" network
openstack endpoint create --region RegionOne \
  network public http://controller:9696
openstack endpoint create --region RegionOne \
  network internal http://controller:9696
openstack endpoint create --region RegionOne \
  network admin http://controller:9696

#option 1

apt-get install neutron-server neutron-plugin-ml2 \
  neutron-linuxbridge-agent neutron-dhcp-agent \
  neutron-metadata-agent
#editar /etc/neutron/neutron.conf
[database]
connection = mysql+pymysql://neutron:neutron@controller/neutron
[DEFAULT]
core_plugin = ml2
service_plugins =
[DEFAULT]
rpc_backend = rabbit
[oslo_messaging_rabbit]
rabbit_host = controller
rabbit_userid = openstack
rabbit_password = openstack
[DEFAULT]
auth_strategy = keystone
[keystone_authtoken]
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = neutron
[DEFAULT]
notify_nova_on_port_status_changes = True
notify_nova_on_port_data_changes = True
[nova]
auth_url = http://controller:35357
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = nova
password = nova

#editar /etc/neutron/plugins/ml2/ml2_conf.ini
[ml2]
type_drivers = flat,vlan
tenant_network_types =
mechanism_drivers = linuxbridge
extension_drivers = port_security
[ml2_type_flat]
flat_networks = provider
[securitygroup]
enable_ipset = True

#editar /etc/neutron/plugins/ml2/linuxbridge_agent.ini
# VER PROVIDER_INTERFACE_NAME eth0 ?????
[linux_bridge]
physical_interface_mappings = provider:eth0
[vxlan]
enable_vxlan = False
[securitygroup]
enable_security_group = True
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver

#editar /etc/neutron/dhcp_agent.ini
[DEFAULT]
interface_driver = neutron.agent.linux.interface.BridgeInterfaceDriver
dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
enable_isolated_metadata = True

#editar /etc/neutron/metadata_agent.ini
[DEFAULT]
nova_metadata_ip = controller
metadata_proxy_shared_secret = e47ca2e155efe75c9fec

#editar /etc/nova/nova.conf
[neutron]
url = http://controller:9696
auth_url = http://controller:35357
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = neutron
password = neutron
service_metadata_proxy = True
metadata_proxy_shared_secret = e47ca2e155efe75c9fec

su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf \
  --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron

service nova-api restart
service neutron-server restart
service neutron-linuxbridge-agent restart
service neutron-dhcp-agent restart
service neutron-metadata-agent restart

neutron ext-list
neutron agent-list


## dashboard
apt-get install openstack-dashboard

#editar /etc/openstack-dashboard/local_settings.py
OPENSTACK_HOST = "controller"
ALLOWED_HOSTS = ['*', ]
#memcached
SESSION_ENGINE = 'django.contrib.sessions.backends.cache'
CACHES = {
    'default': {
         'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',
         'LOCATION': 'controller:11211',
    }
}

OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST
OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True
OPENSTACK_API_VERSIONS = {
    "identity": 3,
    "image": 2,
    "volume": 2,
}
OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = "default"
OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"
OPENSTACK_NEUTRON_NETWORK = {
    ...
    'enable_router': False,
    'enable_quotas': False,
    'enable_distributed_router': False,
    'enable_ha_router': False,
    'enable_lb': False,
    'enable_firewall': False,
    'enable_vpn': False,
    'enable_fip_topology_check': False,
}
COMPRESS_OFFLINE = False

service apache2 reload

#bug https://bugs.launchpad.net/horizon/+bug/1573488
#editar /etc/apache2/conf-enabled/openstack-dashboard.conf
WSGIScriptAlias /horizon /usr/share/openstack-dashboard/openstack_dashboard/wsgi/django.wsgi
WSGIDaemonProcess horizon user=horizon group=horizon processes=3 threads=10
WSGIProcessGroup horizon
WSGIApplicationGroup %{GLOBAL}
Alias /static /usr/share/openstack-dashboard/openstack_dashboard/static/
Alias /horizon/static /usr/share/openstack-dashboard/openstack_dashboard/static/
<Directory /usr/share/openstack-dashboard/openstack_dashboard/wsgi>
  Order allow,deny
  Allow from all
</Directory>

#acessar http://controller/horizon


## storage on controller node
mysql -u root -p
CREATE DATABASE cinder;
GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' \
  IDENTIFIED BY 'cinder';
GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' \
  IDENTIFIED BY 'cinder';

. admin-openrc.sh
openstack user create --domain default --password-prompt cinder
#senha
openstack role add --project service --user cinder admin
openstack service create --name cinder \
  --description "OpenStack Block Storage" volume
openstack service create --name cinderv2 \
  --description "OpenStack Block Storage" volumev2
openstack endpoint create --region RegionOne \
  volume public http://controller:8776/v1/%\(tenant_id\)s
openstack endpoint create --region RegionOne \
  volume internal http://controller:8776/v1/%\(tenant_id\)s
openstack endpoint create --region RegionOne \
  volume admin http://controller:8776/v1/%\(tenant_id\)s
openstack endpoint create --region RegionOne \
  volumev2 public http://controller:8776/v2/%\(tenant_id\)s
openstack endpoint create --region RegionOne \
  volumev2 internal http://controller:8776/v2/%\(tenant_id\)s
openstack endpoint create --region RegionOne \
  volumev2 admin http://controller:8776/v2/%\(tenant_id\)s

apt-get install cinder-api cinder-scheduler

#editar /etc/cinder/cinder.conf
[database]
connection = mysql+pymysql://cinder:cinder@controller/cinder
[DEFAULT]
rpc_backend = rabbit
[oslo_messaging_rabbit]
rabbit_host = controller
rabbit_userid = openstack
rabbit_password = openstack
[DEFAULT]
auth_strategy = keystone
[keystone_authtoken]
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = cinder
password = cinder
[DEFAULT]
my_ip = 192.168.2.8
[oslo_concurrency]
lock_path = /var/lib/cinder/tmp

su -s /bin/sh -c "cinder-manage db sync" cinder

#editar /etc/nova/nova.conf
[cinder]
os_region_name = RegionOne

service nova-api restart
service cinder-scheduler restart
service cinder-api restart

cinder service-list


############ compute+storage node
apt-get install software-properties-common
add-apt-repository cloud-archive:mitaka
apt-get update
apt-get install python-openstackclient


## compute on compute node
apt-get install nova-compute

#editar /etc/nova/nova.conf
[DEFAULT]
rpc_backend = rabbit
[oslo_messaging_rabbit]
rabbit_host = controller
rabbit_userid = openstack
rabbit_password = openstack
[DEFAULT]
auth_strategy = keystone
[keystone_authtoken]
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = nova
[DEFAULT]
my_ip = 192.168.2.9
[DEFAULT]
use_neutron = True
firewall_driver = nova.virt.firewall.NoopFirewallDriver
[vnc]
enabled = True
vncserver_listen = 0.0.0.0
vncserver_proxyclient_address = $my_ip
novncproxy_base_url = http://controller:6080/vnc_auto.html
[glance]
api_servers = http://controller:9292
[oslo_concurrency]
lock_path = /var/lib/nova/tmp
#remover logdir

#editar /etc/nova/nova-compute.conf
[libvirt]
virt_type = qemu

service nova-compute restart


## storage on compute+storage node
apt-get install cinder-volume

#editar /etc/cinder/cinder.conf
[database]
connection = mysql+pymysql://cinder:cinder@controller/cinder
[DEFAULT]
rpc_backend = rabbit
[oslo_messaging_rabbit]
rabbit_host = controller
rabbit_userid = openstack
rabbit_password = openstack
[DEFAULT]
auth_strategy = keystone
[keystone_authtoken]
auth_uri = http://controller:5000
auth_url = http://controller:35357
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = cinder
password = cinder
[DEFAULT]
my_ip = 192.168.2.9
#[lvm]
#volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver
#volume_group = cinder-volumes
#iscsi_protocol = iscsi
#iscsi_helper = tgtadm
[DEFAULT]
#enabled_backends = lvm
[DEFAULT]
glance_api_servers = http://controller:9292
[oslo_concurrency]
lock_path = /var/lib/cinder/tmp

service tgt restart
service cinder-volume restart


#############################
## CEPH ##
#############################

#http://blog.programster.org/ubuntu-14-04-deploy-a-ceph-cluster-part-1/
#http://docs.ceph.com/docs/master/start/quick-ceph-deploy/
#http://www.rajtechtips.com/2014/06/create-2-node-ceph-storage-cluster/

## ceph cluster - storage nodes
wget -q -O- 'https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc' | sudo apt-key add -
echo deb http://download.ceph.com/debian-jewel/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list
apt update
apt install ceph-deploy

apt-get install ntp openssh-server -y

#cria usuario para ceph deploy
USERNAME="ceph_deploy"  
sudo useradd -d /home/$USERNAME -m $USERNAME  
sudo passwd $USERNAME  
#senha ceph_deploy
visudo
ceph_deploy ALL=(ALL) NOPASSWD:ALL

#node 1
sudo -H -u ceph_deploy bash
ssh-keygen
ssh-copy-id ceph_deploy@bruno2

#node 2
sudo -H -u ceph_deploy bash
ssh-keygen
ssh-copy-id ceph_deploy@brunonote

#node deploy node 2
sudo -H -u ceph_deploy bash
mkdir ~/ceph_cluster
cd ~/ceph_cluster

ceph-deploy new bruno2 #monitor
vi ceph.conf
[global]
osd pool default size = 2

ceph-deploy install brunonote bruno2

ceph-deploy mon create-initial

ssh brunonote sudo mkdir /var/local/osd0
sudo mkdir /var/local/osd1

#formatar particao em Linux 83 e montar
fdisk /dev/sdX
mkfs.xfs /dev/sdXn -f
#colocar em /etc/fstab
mount /dev/sdXn

ssh brunonote sudo chown ceph /var/local/osd0/
sudo chown ceph /var/local/osd1/

ceph-deploy osd prepare \
brunonote:/var/local/osd0 \
bruno2:/var/local/osd1

ceph-deploy osd activate \
brunonote:/var/local/osd0 \
bruno2:/var/local/osd1

ceph-deploy admin \
brunonote \
bruno2

sudo chmod +r /etc/ceph/ceph.client.admin.keyring

ceph-deploy mon add bruno2

chown ceph /var/log/ceph/

ceph health


#http://docs.ceph.com/docs/master/rbd/rbd-openstack/?highlight=openstack#create-a-pool
#http://ceph.com/planet/openstack-nova-configure-multiple-ceph-backends-on-one-hypervisor/

## ceph+openstack on controller node
#sudo apt-get install python-rbd #instalado no deploy

#keys
ceph auth get-or-create client.cinder mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rx pool=images'
ceph auth get-or-create client.glance mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=images'
ceph auth get-or-create client.glance | sudo tee /etc/ceph/ceph.client.glance.keyring
sudo chown glance:glance /etc/ceph/ceph.client.glance.keyring
ceph auth get-or-create client.cinder | sudo tee /etc/ceph/ceph.client.cinder.keyring
sudo chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring
ceph auth get-or-create client.cinder | ssh 192.168.2.9 sudo tee /etc/ceph/ceph.client.cinder.keyring
ssh 192.168.2.9 sudo chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring

ceph auth get-key client.cinder | tee client.cinder.key
UUIDRAND=$(uuidgen)
cat > secret.xml <<EOF
<secret ephemeral='no' private='no'>
  <uuid>$UUIDRAND</uuid>
  <usage type='ceph'>
    <name>client.cinder secret</name>
  </usage>
</secret>
EOF
sudo virsh secret-define --file secret.xml
sudo virsh secret-set-value --secret $UUIDRAND --base64 $(cat client.cinder.key) && rm client.cinder.key secret.xml

ceph auth get-key client.cinder | ssh 192.168.2.9 tee client.cinder.key
ssh 192.168.2.9 tee secret.xml <<EOF
<secret ephemeral='no' private='no'>
  <uuid>$UUIDRAND</uuid>
  <usage type='ceph'>
    <name>client.cinder secret</name>
  </usage>
</secret>
EOF
ssh 192.168.2.9 sudo virsh secret-define --file secret.xml
ssh 192.168.2.9 sudo virsh secret-set-value --secret $UUIDRAND --base64 $(ssh 192.168.2.9 cat client.cinder.key)
ssh 192.168.2.9 rm client.cinder.key secret.xml


#editar /etc/glance/glance-api.conf
[DEFAULT]
show_image_direct_url = True
default_store = rbd
[glance_store]
stores = rbd,file  # adicinei file para permitir local
rbd_store_pool = images
rbd_store_user = glance
rbd_store_ceph_conf = /etc/ceph/ceph.conf
rbd_store_chunk_size = 8


## ceph on compute+storage node
#sudo apt-get install ceph-common #instalado no deploy

ceph osd pool create images 128
ceph osd pool create volumes 128
ceph osd pool create vms 128
#ceph osd pool create backups ?

#editar /etc/cinder/cinder.conf
[DEFAULT]
enabled_backends = ceph
[ceph]
volume_driver = cinder.volume.drivers.rbd.RBDDriver
rbd_pool = volumes
rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_flatten_volume_from_snapshot = false
rbd_max_clone_depth = 5
rbd_store_chunk_size = 4
rados_connect_timeout = -1
glance_api_version = 2
rbd_user = cinder
rbd_secret_uuid = c843b8ed-f81a-4bd7-9450-515199bb062d   #uuid do compute nodes libvirt

#cache opcional
ceph daemon /var/run/ceph/ceph-client.cinder.19195.32310016.asok help
#editar em compute nodes
[client]
    rbd cache = true
    rbd cache writethrough until flush = true
    admin socket = /var/run/ceph/guests/$cluster-$type.$id.$pid.$cctid.asok
    log file = /var/log/qemu/qemu-guest-$pid.log
    rbd concurrent management ops = 20
mkdir -p /var/run/ceph/guests/ /var/log/qemu/
chown qemu:libvirtd /var/run/ceph/guests /var/log/qemu/

#editar /etc/nova/nova.conf
[libvirt]
images_type = rbd
images_rbd_pool = vms
images_rbd_ceph_conf = /etc/ceph/ceph.conf
rbd_user = cinder
rbd_secret_uuid = c843b8ed-f81a-4bd7-9450-515199bb062d   #uuid do compute nodes libvirt
inject_password = false
inject_key = false
inject_partition = -2
live_migration_flag="VIR_MIGRATE_UNDEFINE_SOURCE,VIR_MIGRATE_PEER2PEER,VIR_MIGRATE_LIVE,VIR_MIGRATE_PERSIST_DEST,VIR_MIGRATE_TUNNELLED"

sudo glance-control api restart
sudo service nova-compute restart
sudo service cinder-volume restart


## VMS
## controller node no openstack
neutron net-create --shared --provider:physical_network provider   --provider:network_type flat provider

neutron subnet-create --name provider \
  --allocation-pool start=192.168.2.18,end=192.168.2.22 \
  --dns-nameserver 186.195.16.15 --gateway 192.168.2.1 \
  provider 192.168.2.0/24

openstack image create Ubuntu-server-16.04 --file /var/lib/libvirt/images/ubuntu-16.04-server-amd64.iso \
  --disk-format iso

cinder create --image Ubuntu-server-16.04 --display-name vm1 5

openstack server create --flavor m1.tiny --volume vm1 \
  --nic net-id=provider vm1
#OU
#no horizon: intance - launch instance, nome, origem inicializacao volume -> escolher o volume criado, escolher tipo e rede
#nao completa instalacao do ubuntu


#copiar disco ja existente
openstack image create teste1 --file /var/lib/libvirt/images/HAteste1.img --disk-format raw

cinder create --image teste1 --display-name vm4 5

openstack server create --flavor m1.tiny --volume vm4 \
  --nic net-id=provider vm4

#live migration
#bug cpu
#https://blog.codecentric.de/en/2015/03/true-kvm-live-migration-openstack-icehouse-ceph-based-vm-storage/
#https://bugs.launchpad.net/nova/+bug/1082414/comments/29
#ver qual cpu tem na maioria
sudo virsh capabilities | xmllint --xpath "/capabilities/host/cpu" - > /tmp/$(hostname).xml' 
cat /tmp/node*.xml >> /tmp/all-cpus.xml  
sudo virsh cpu-baseline /tmp/all-cpus.xml

cpu_model=Penryn
cpu_mode=None
virt_type=qemu
em /etc/nova/nova.conf


