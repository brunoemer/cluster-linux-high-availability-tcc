#tentativa 1
http://linux.opm.si/programska-oprema/linux-gruca-cluster

sudo apt-get install lsscsi sysstat iftop iptraf iputils-arping bridge-utils ifenslave drbd8-utils drbdlinks qemu-kvm ubuntu-vm-builder libvirt-bin virt-viewer ntp pacemaker pacemaker-mgmt corosync resource-agents fence-agents 
# nao funcionou sem cman

#levantar drbd
#/etc/drbd.d/vms.conf
resource vms {
    meta-disk internal;
    device /dev/drbd0;
    on brunonote {
        address 192.168.2.8:7791;
        disk /dev/vgdrbd/vms;
    }
    on bruno2 {
        address 192.168.2.9:7791;
        disk /dev/vgdrbd/vms;
    }
}

#primary
drbdadm up vms
drbdadm primary vms
mount /dev/drbd0 /var/lib/libvirt/images/drbd/

#secundary
drbdadm up vms

#resincronizar secundary
drbdadm down vms
drbdadm -- --discard-my-data connect vms

##########################

#tentativa 2
https://velenux.wordpress.com/2015/05/26/creating-a-corosync-2-x-pacemaker-1-1-cluster-on-ubuntu-14-04-lts/
http://zeldor.biz/2010/12/activepassive-cluster-with-pacemaker-corosync/

apt-get install pacemaker corosync rsync screen vim-nox mutt curl wget sysstat ntp

#alterar para
/etc/default/corosync
START=yes

#no cluster2 bruno2 colocar ip dele no nodelist antes ring0_addr: 192.168.2.9

service corosync start
service pacemaker start

#startup
for SRV in corosync pacemaker ; do
    update-rc.d $SRV defaults
    service $SRV start
done

crm configure property stonith-enabled=false
crm configure property no-quorum-policy=ignore
#para nao retornar apos queda
crm configure rsc_defaults resource-stickiness=100

#funcionou com
/etc/corosync/corosync.conf
# Please read the openais.conf.5 manual page

totem {
	version: 2

	# How long before declaring a token lost (ms)
	token: 3000

	# How many token retransmits before forming a new configuration
	token_retransmits_before_loss_const: 10

	# How long to wait for join messages in the membership protocol (ms)
	join: 60

	# How long to wait for consensus to be achieved before starting a new round of membership configuration (ms)
	consensus: 3600

	# Turn off the virtual synchrony filter
	vsftype: none

	# Number of messages that may be sent by one processor on receipt of the token
	max_messages: 20

	# Limit generated nodeids to 31-bits (positive signed integers)
	clear_node_high_bit: yes

	# Disable encryption
 	secauth: off

	# How many threads to use for encryption/decryption
 	threads: 0

	# Optionally assign a fixed node id (integer)
	# nodeid: 1234

	# This specifies the mode of redundant ring, which may be none, active, or passive.
 	rrp_mode: none

 	interface {
		# The following values need to be set based on your environment 
		ringnumber: 0
		bindnetaddr: 192.168.2.0 
		mcastaddr: 226.94.1.1
		mcastport: 5405
	}
}

amf {
	mode: disabled
}

quorum {
	# Quorum for the Pacemaker Cluster Resource Manager
	provider: corosync_votequorum
	expected_votes: 2
}

# nodes
nodelist {
        node { 
                ring0_addr: 192.168.2.8
        }
        node { 
                ring0_addr: 192.168.2.9
        }
}

service {
        # Load the Pacemaker Cluster Resource Manager
        ver:       0
        name:      pacemaker
}

aisexec {
        user:   root
        group:  root
}

logging {
        fileline: off
        to_stderr: yes
        to_logfile: no
        to_syslog: yes
	syslog_facility: daemon
        debug: off
        timestamp: on
        logger_subsys {
                subsys: AMF
                debug: off
                tags: enter|leave|trace1|trace2|trace3|trace4|trace6
        }
}

#configurar as primitivas do cluster
# DRBD:
crm configure primitive DRBD ocf:linbit:drbd \
        params drbd_resource="vms" \
        op monitor interval="20" role="Master" timeout="20" \
        op monitor interval="30" role="Slave" timeout="20" \
        meta is-managed="true"
# DRBD primary secondary
crm configure ms MS_DRBD DRBD meta master-max="1" clone-max="2" clone-node-max="1" notify="true" target-role="Master"

# file system:
crm configure primitive LIBVIRT_FS ocf:heartbeat:Filesystem params device="/dev/drbd0" directory="/var/lib/libvirt/images/drbd" fstype="ext4"

# virtual domain
crm configure primitive VMS ocf:heartbeat:VirtualDomain \
    params config="/etc/libvirt/qemu/HAteste1.xml" \
    hypervisor="qemu:///system" \
    op start timeout="20s" \
    op stop timeout="20s" \
    op monitor depth="0" timeout="15" interval="10" \
    meta allow-migrate="false"


#nodo com preferencia.
crm configure location master-prefer-node1 VMS 50: brunonote

#configuracao final
node $id="1084752392" brunonote
node $id="1084752393" bruno2
primitive DRBD ocf:linbit:drbd \
        params drbd_resource="vms" \
        op monitor interval="20" role="Master" timeout="20" \
        op monitor interval="30" role="Slave" timeout="20" \
        meta is-managed="true"
primitive LIBVIRT_FS ocf:heartbeat:Filesystem \
        params device="/dev/drbd0" directory="/var/lib/libvirt/images/drbd" fstype="ext4" \
        meta is-managed="true" target-role="Started"
primitive VMS ocf:heartbeat:VirtualDomain \
        params config="/etc/libvirt/qemu/HAteste1.xml" hypervisor="qemu:///system" \
        op start timeout="20s" interval="0" \
        op stop timeout="20s" interval="0" \
        op monitor timeout="15" interval="10" depth="0" \
        meta allow-migrate="false" target-role="Started" \
        utilization cpu="1" hv_memory="512"
ms MS_DRBD DRBD \
        meta master-max="1" clone-max="2" clone-node-max="1" notify="true" target-role="Master"
location cli-prefer LIBVIRT_FS 100: bruno2
location master-prefer-node1 LIBVIRT_FS 50: brunonote
colocation col_DRBD_FS inf: LIBVIRT_FS MS_DRBD:Master
colocation col_DRBD_VMS inf: VMS MS_DRBD:Master
order ord_DRBD_VMS inf: MS_DRBD:promote LIBVIRT_FS:start VMS:start
property $id="cib-bootstrap-options" \
        dc-version="1.1.10-42f2063" \
        cluster-infrastructure="corosync" \
        stonith-enabled="false" \
        no-quorum-policy="ignore"
rsc_defaults $id="rsc-options" \
        resource-stickiness="100"

# migrar de brunonote para bruno2
crm resource migrate MS_DRBD bruno2
crm resource restart MS_DRBD

# funcionou com uma vm desligando e ligando no nó primario

# interface java para gerencia
http://lcmc.sourceforge.net/



####################################
# TESTE PRIMARY SECUNDARY SEM LIVE MIGRATION

#filtro no lvm - /etc/lvm/lvm.conf
# By default we accept every block device:
filter = [ "a|.*|", "r|/dev/drbd[0-9]+|" ]

# vg drbd
#formatar particao em lvm2 e criar pv
vgcreate vgdrbd /dev/sda10
lvcreate -n vm1 vgdrbd -L 5G

# editar /etc/drbd.d/vm1.res
resource vm1 {
    meta-disk internal;
    device /dev/drbd0;
    on brunonote.emer.com {
        address 192.168.2.8:7791;
        disk /dev/vgdrbd/vm1;
    }
    on bruno2.emer.com {
        address 192.168.2.9:7791;
        disk /dev/vgdrbd/vm1;
    }
}

service drbd reload

#primary
drbdadm up vm1
drbdadm create-md vm1
#drbdadm -- --overwrite-data-of-peer primary vm1  #primeira vez
drbdadm primary vm1

#secundary
drbdadm up vm1
drbdadm create-md vm1
#resincronizar secundary
drbdadm -- --discard-my-data connect vm1

# verificar
root@brunonote:~# service drbd status
drbd driver loaded OK; device status:
version: 8.4.5 (api:1/proto:86-101)
srcversion: 5A4F43804B37BB28FCB1F47 
m:res  cs         ro                 ds                 p  mounted  fstype
0:vm1  Connected  Primary/Secondary  UpToDate/UpToDate  C

#importar imagem existente
dd if=/var/lib/libvirt/images/HAteste1.img of=/dev/drbd/by-res/vm1 bs=1M

#criar vm libvirt
#pool
virsh pool-create-as vgdrbd --type=logical --target=/dev/vgdrbd
#xml vm
<domain type='kvm'>
  <name>vm1</name>
  <uuid>62d4fb34-1c0e-07e6-3264-89ce12f6ef25</uuid>
  <memory unit='KiB'>524288</memory>
  <currentMemory unit='KiB'>524288</currentMemory>
  <vcpu placement='static'>1</vcpu>
  <os>
    <type arch='x86_64' machine='pc-i440fx-trusty'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/bin/kvm-spice</emulator>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/vgdrbd/vm1'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
    <controller type='usb' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x2'/>
    </controller>
    <controller type='pci' index='0' model='pci-root'/>
    <interface type='network'>
      <mac address='52:54:00:3b:df:98'/>
      <source network='default'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <input type='keyboard' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <sound model='ich6'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </memballoon>
  </devices>
</domain>

scp /home/bruno/vm1.xml bruno2:/home/bruno/
ssh bruno2 virsh define /home/bruno/vm1.xml

# corosync /etc/corosync/corosync.conf
totem {
	version: 2

	# How long before declaring a token lost (ms)
	token: 3000

	# How many token retransmits before forming a new configuration
	token_retransmits_before_loss_const: 10

	# How long to wait for join messages in the membership protocol (ms)
	join: 60

	# How long to wait for consensus to be achieved before starting a new round of membership configuration (ms)
	consensus: 3600

	# Turn off the virtual synchrony filter
	vsftype: none

	# Number of messages that may be sent by one processor on receipt of the token
	max_messages: 20

	# Limit generated nodeids to 31-bits (positive signed integers)
	clear_node_high_bit: yes

	# Disable encryption
 	secauth: off

	# How many threads to use for encryption/decryption
 	threads: 0

	# Optionally assign a fixed node id (integer)
	# nodeid: 1234

	# This specifies the mode of redundant ring, which may be none, active, or passive.
 	rrp_mode: none

 	interface {
		# The following values need to be set based on your environment 
		ringnumber: 0
		bindnetaddr: 192.168.2.0 
		mcastaddr: 226.94.1.1
		mcastport: 5405
	}
}

amf {
	mode: disabled
}

quorum {
	# Quorum for the Pacemaker Cluster Resource Manager
	provider: corosync_votequorum
	expected_votes: 2
}

# nodes
nodelist {
        node { 
                ring0_addr: 192.168.2.8
        }
        node { 
                ring0_addr: 192.168.2.9
        }
}

service {
        # Load the Pacemaker Cluster Resource Manager
        ver:       0
        name:      pacemaker
}

aisexec {
        user:   root
        group:  root
}

logging {
        fileline: off
        to_stderr: yes
        to_logfile: no
        to_syslog: yes
	syslog_facility: daemon
        debug: off
        timestamp: on
        logger_subsys {
                subsys: AMF
                debug: off
                tags: enter|leave|trace1|trace2|trace3|trace4|trace6
        }
}

service corosync restart
service pacemaker restart

crm status
crm resource show

#configuracao
crm configure edit

node $id="1084752392" brunonote.emer.com \
        attributes maintenance="off" standby="off"
node $id="1084752393" bruno2.emer.com \
        attributes standby="off" maintenance="off"
primitive DRBD ocf:linbit:drbd \
        params drbd_resource="vm1" \
        op monitor interval="20" role="Master" timeout="240" \
        op monitor interval="30" role="Slave" timeout="240" \
        meta is-managed="true"
primitive VM1 ocf:heartbeat:VirtualDomain \
        params config="/etc/libvirt/qemu/vm1.xml" hypervisor="qemu:///system" migration_transport="ssh" force_stop="0" \
        op start timeout="90" interval="0" \
        op stop timeout="90" interval="0" \
        op monitor timeout="30" interval="10" depth="0" \
        op migrate_from interval="0" timeout="240" \
        op migrate_to interval="0" timeout="240" \
        meta allow-migrate="true" target-role="Started" \
        utilization cpu="1" hv_memory="512"
ms MS_DRBD DRBD \
        meta master-max="1" clone-max="2" clone-node-max="1" notify="true" target-role="Started" \
	migration-threshold="1" allow-migrate="true"
location cli-prefer-MS_DRBD MS_DRBD inf: bruno2.emer.com
colocation col_DRBD_VMS inf: VM1 MS_DRBD:Master
order ord_DRBD_VMS inf: MS_DRBD:promote VM1:start
property $id="cib-bootstrap-options" \
        dc-version="1.1.10-42f2063" \
        cluster-infrastructure="corosync" \
        stonith-enabled="false" \
        no-quorum-policy="ignore" \
        last-lrm-refresh="1466773789"
rsc_defaults $id="rsc-options" \
        resource-stickiness="100" \
        migration-threshold="3"

#para aplicar reiniciar os nodes
reboot


####################################
# TESTE LIVE MIGRATION E DUAL PRIMARY DRBD + CLVM
# live migration com lvm
https://rarforge.com/w/index.php/2_Node_Cluster:_Dual_Primary_DRBD_%2B_CLVM_%2B_KVM_%2B_Live_Migrations#Pacemaker

#CLVM
apt-get install clvm
apt-get install libdlmcontrol3
lvmconf --enable-cluster
#filtro no lvm - /etc/lvm/lvm.conf
devices {
	filter = ["a|sd.*|", "a|drbd.*|", "r|.*|"]
	write_cache_state = 0
	locking_type = 3
	fallback_to_local_locking = 0
}
update-rc.d -f clvm remove
### nao funcionou com clvm pois precisa do cman

# vg drbd
#formatar particao em lvm2 e criar pv
vgcreate vgdrbd /dev/sda10
lvcreate -n lvdrbd vgdrbd -L 6,5G   

# editar /etc/drbd.d/vms.res
resource vms {
    meta-disk internal;
    device /dev/drbd0;
    protocol C;

    startup {
        become-primary-on both;
        wfc-timeout 40;
        degr-wfc-timeout 240;
    }
    net {
        allow-two-primaries;
        after-sb-0pri discard-zero-changes;
        after-sb-1pri discard-secondary;
        after-sb-2pri disconnect;
    }
    disk {
        on-io-error detach;
        fencing resource-and-stonith;
    }
    handlers {
        #split-brain "/usr/lib/drbd/notify-split-brain.sh root";
    }
    syncer {
        rate 50M;
    }
    on brunonote.emer.com {
        address 192.168.2.8:7791;
        disk /dev/vgdrbd/lvdrbd;
    }
    on bruno2.emer.com {
        address 192.168.2.9:7791;
        disk /dev/vgdrbd/lvdrbd;
    }
}

service drbd reload

drbdadm create-md vms
drbdadm up vms
drbdadm primary vms
#drbdadm -- --overwrite-data-of-peer primary vms  #primeira vez

# verificar
root@brunonote:~# service drbd status
drbd driver loaded OK; device status:
version: 8.4.5 (api:1/proto:86-101)
srcversion: 5A4F43804B37BB28FCB1F47 
m:res  cs         ro               ds                 p  mounted  fstype
0:vms  Connected  Primary/Primary  UpToDate/UpToDate  C

# vg cluster
pvcreate /dev/drbd0
vgcreate --clustered y vgdrbdcluster /dev/drbd0 # em apenas um node
lvcreate -n lvvm1 -L 6G vgdrbdcluster
#NAO FUNCINOU NO UBUNTU VG CLUSTERED

#importar imagem existente
dd if=/var/lib/libvirt/images/HAteste1.img of=/dev/vgdrbdcluster/lvvm1 bs=1M

#criar vm libvirt
#pool
virsh pool-create-as vgdrbd --type=logical --target=/dev/vgdrbdcluster/
#xml vm
<domain type='kvm'>
  <name>vm1</name>
  <uuid>62d4fb34-1c0e-07e6-3264-89ce12f6ef25</uuid>
  <memory unit='KiB'>524288</memory>
  <currentMemory unit='KiB'>524288</currentMemory>
  <vcpu placement='static'>1</vcpu>
  <os>
    <type arch='x86_64' machine='pc-i440fx-trusty'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/bin/kvm-spice</emulator>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source dev='/dev/vgdrbdcluster/lvvm1'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
    <controller type='usb' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x2'/>
    </controller>
    <controller type='pci' index='0' model='pci-root'/>
    <interface type='network'>
      <mac address='52:54:00:3b:df:98'/>
      <source network='default'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <input type='keyboard' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <sound model='ich6'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </memballoon>
  </devices>
</domain>

scp /home/bruno/vm1.xml bruno2:/home/bruno/
ssh bruno2 virsh define /home/bruno/vm1.xml

# corosync /etc/corosync/corosync.conf
totem {
	version: 2

	# How long before declaring a token lost (ms)
	token: 3000

	# How many token retransmits before forming a new configuration
	token_retransmits_before_loss_const: 10

	# How long to wait for join messages in the membership protocol (ms)
	join: 60

	# How long to wait for consensus to be achieved before starting a new round of membership configuration (ms)
	consensus: 3600

	# Turn off the virtual synchrony filter
	vsftype: none

	# Number of messages that may be sent by one processor on receipt of the token
	max_messages: 20

	# Limit generated nodeids to 31-bits (positive signed integers)
	clear_node_high_bit: yes

	# Disable encryption
 	secauth: off

	# How many threads to use for encryption/decryption
 	threads: 0

	# Optionally assign a fixed node id (integer)
	# nodeid: 1234

	# This specifies the mode of redundant ring, which may be none, active, or passive.
 	rrp_mode: none

 	interface {
		# The following values need to be set based on your environment 
		ringnumber: 0
		bindnetaddr: 192.168.2.0 
		mcastaddr: 226.94.1.1
		mcastport: 5405
	}
}

amf {
	mode: disabled
}

quorum {
	# Quorum for the Pacemaker Cluster Resource Manager
	provider: corosync_votequorum
	expected_votes: 2
}

# nodes
nodelist {
        node { 
                ring0_addr: 192.168.2.8
        }
        node { 
                ring0_addr: 192.168.2.9
        }
}

service {
        # Load the Pacemaker Cluster Resource Manager
        ver:       0
        name:      pacemaker
}

aisexec {
        user:   root
        group:  root
}

logging {
        fileline: off
        to_stderr: yes
        to_logfile: no
        to_syslog: yes
	syslog_facility: daemon
        debug: off
        timestamp: on
        logger_subsys {
                subsys: AMF
                debug: off
                tags: enter|leave|trace1|trace2|trace3|trace4|trace6
        }
}

service corosync restart
service pacemaker restart

crm status
crm resource show

#configuracao
crm configure edit

node $id="1084752392" brunonote.emer.com \
        attributes maintenance="off" standby="off"
node $id="1084752393" bruno2.emer.com \
        attributes standby="off" maintenance="off"
primitive DRBD ocf:linbit:drbd \
        params drbd_resource="vm1" \
        op monitor interval="20" role="Master" timeout="240" \
        op monitor interval="30" role="Slave" timeout="240" \
        op start interval="0" timeout="240" \
        op stop interval="0" timeout="100" start-delay="0" \
        meta is-managed="true"
primitive VM1 ocf:heartbeat:VirtualDomain \
        params config="/etc/libvirt/qemu/vm1.xml" hypervisor="qemu:///system" migration_transport="ssh" force_stop="0" \
        op start timeout="90" interval="0" \
        op stop timeout="90" interval="0" \
        op monitor timeout="30" interval="10" depth="0" \
        op migrate_from interval="0" timeout="240" \
        op migrate_to interval="0" timeout="240" \
        meta allow-migrate="true" target-role="Started" \
        utilization cpu="1" hv_memory="512"
ms MS_DRBD DRBD \
        meta master-max="2" clone-max="2" clone-node-max="1" notify="true" target-role="Master" migration-threshold="1" allow-migrate="true" is-managed="true"
location cli-prefer-MS_DRBD MS_DRBD inf: brunonote.emer.com
location cli-prefer-VM1 VM1 inf: brunonote.emer.com
colocation col_DRBD_VMS inf: VM1 MS_DRBD:Master
order ord_DRBD_VMS inf: MS_DRBD:promote VM1:start
property $id="cib-bootstrap-options" \
        dc-version="1.1.10-42f2063" \
        cluster-infrastructure="corosync" \
        stonith-enabled="true" \
        no-quorum-policy="ignore" \
        last-lrm-refresh="1471391828" \
	expected-quorum-votes="2" \
	stonith-action="poweroff"
rsc_defaults $id="rsc-options" \
        resource-stickiness="100" \
        migration-threshold="3"

#para aplicar reiniciar os nodes
reboot

#manutencao vms
#live migration com drbd dual primary
virsh migrate --live vm1 qemu+ssh://bruno2.emer.com/system
#OK, mas nao funciona caso um node falhar

crm resource migrate MS_DRBD brunonote.emer.com
# nao funcionou

crm node maintenance brunonote.emer.com
service pacemaker stop
service corosync stop
###### split brain

#para remover gerencia pacemaker para fazer manualmente
crm resource meta MS_DRBD set is-managed false

#desativar cluster
crm resource stop MS_DRBD
crm resource start MS_DRBD
crm resource cleanup MS_DRBD

##### NAO FUNCIONOU, APENAS LIVE MIGRATION SEM CLVM


################################
# TESTE LIVE MIGRATION E DUAL PRIMARY DRBD + OCFS2
http://publications.jbfavre.org/virtualisation/cluster-xen-corosync-pacemaker-drbd-ocfs2.en

# vg drbd
#formatar particao em lvm2 e criar pv
pvcreate /dev/sda10
vgcreate vgdrbd /dev/sda10
lvcreate -n lvdrbd vgdrbd -L 6,5G

# editar /etc/drbd.d/global_common.conf
global {
        usage-count yes;
        minor-count 16;
}
# editar /etc/drbd.d/vms.res
resource vms {
    meta-disk internal;
    device /dev/drbd0;
    protocol C;
    disk {
        fencing resource-only;
    }
    handlers {
        fence-peer "/usr/lib/drbd/crm-fence-peer.sh";
        after-resync-target "/usr/lib/drbd/crm-unfence-peer.sh";
    }
    net {
        allow-two-primaries;
    }
    startup {
        become-primary-on both;
    }
    on brunonote.emer.com {
        address 192.168.2.8:7791;
        disk /dev/vgdrbd/lvdrbd;
    }
    on bruno2.emer.com {
        address 192.168.2.9:7791;
        disk /dev/vgdrbd/lvdrbd;
    }
}

apt-get install ocfs2-tools
#editar /etc/ocfs2/cluster.conf
node:
        ip_port = 7777
        ip_address = 192.168.2.8
        number = 0
        name = brunonote.emer.com
        cluster = clusterocfs

node:
        ip_port = 7777
        ip_address = 192.168.2.9
        number = 1
        name = bruno2.emer.com
        cluster = clusterocfs

cluster:
        node_count = 2
        name = clusterocfs

#editar /etc/default/o2cb
# O2CB_ENABLED: 'true' means to load the driver on boot.
O2CB_ENABLED=true

# O2CB_BOOTCLUSTER: If not empty, the name of a cluster to start.
O2CB_BOOTCLUSTER=clusterocfs

# O2CB_HEARTBEAT_THRESHOLD: Iterations before a node is considered dead.
O2CB_HEARTBEAT_THRESHOLD=31

# O2CB_IDLE_TIMEOUT_MS: Time in ms before a network connection is considered dead.
O2CB_IDLE_TIMEOUT_MS=30000

# O2CB_KEEPALIVE_DELAY_MS: Max. time in ms before a keepalive packet is sent.
O2CB_KEEPALIVE_DELAY_MS=2000

# O2CB_RECONNECT_DELAY_MS: Min. time in ms between connection attempts.
O2CB_RECONNECT_DELAY_MS=2000

service ocfs2 restart

#carregar o2cb manualmente
service o2cb load
service o2cb online

#configurar load no boot
dpkg-reconfigure ocfs2-tools
#digitar nome do cluster e restante padrao
#criar start boot se nao existir
cd /etc/rcS.d/
ln -s ../init.d/o2cb /etc/rcS.d/S41o2cb

mkfs.ocfs2 /dev/drbd/by-res/vms

###### bug ocfs2 is not compatible with your environment.
# bug fix https://bugs.launchpad.net/ubuntu/+source/ocfs2-tools/+bug/1412438
#patch /usr/lib/ocf/resource.d/heartbeat/Filesystem
--- Filesystem 2013-12-16 07:41:25.000000000 +0000
+++ Filesystem.new 2015-01-19 19:01:30.181772112 +0000
@@ -338,7 +338,7 @@ ocfs2_init()
  # not need this:
  OCFS2_SLES10=""
- if [ "X$HA_cluster_type" = "Xcman" ]; then
+ if [ "X$HA_cluster_type" = "Xcorosync" ]; then
      return
  elif [ "X$HA_cluster_type" != "Xopenais" ]; then
   if grep -q "SUSE Linux Enterprise Server 10" /etc/SuSE-release >/dev/null 2>&1 ; then

apt-get install pacemaker

# corosync /etc/corosync/corosync.conf
totem {
	version: 2
	token: 3000
	token_retransmits_before_loss_const: 10
	join: 60
	consensus: 4320
	vsftype: none
	max_messages: 20
	clear_node_high_bit: yes
 	secauth: on
 	threads: 0
 	rrp_mode: none
        interface {
                # The following values need to be set based on your environment 
                member {
                        memberaddr: 192.168.2.8
                }
                member {
                        memberaddr: 192.168.2.9
                }
                ringnumber: 0
                bindnetaddr: 192.168.2.0
        }
        transport: udpu
}
amf {
	mode: disabled
}
service {
 	ver:       0
 	name:      pacemaker
}
aisexec {
        user:   root
        group:  root
}
logging {
    fileline: off
    to_stderr: yes
    to_logfile: no
    to_syslog: yes
	syslog_facility: daemon
        debug: off
        timestamp: on
        logger_subsys {
            subsys: AMF
            debug: off
            tags: enter|leave|trace1|trace2|trace3|trace4|trace6
        }
}

#node 1
corosync-keygen
scp /etc/corosync/authkey bruno2.emer.com:
#node 2
mv ~/authkey /etc/corosync/authkey
chown root:root /etc/corosync/authkey
chmod 400 /etc/corosync/authkey

#alterar para
/etc/default/corosync
START=yes

mkdir /var/lib/libvirt/images/ocfs

# pacemaker
#adicionar em crm configure edit
node $id="1084752392" brunonote.emer.com
node $id="1084752393" bruno2.emer.com

service corosync restart
service pacemaker restart


##### NAO USAR DLM PELO PACEMAKER https://bugs.launchpad.net/ubuntu/+source/ocfs2-tools/+bug/1412438
primitive p_drbd_ocfs2 ocf:linbit:drbd \
  params drbd_resource="vms"
ms ms_drbd_ocfs2 p_drbd_ocfs2 \
  meta master-max=2 clone-max=2 notify=true
primitive resDRBD ocf:linbit:drbd \
   params drbd_resource="vms" \
   operations $id="resDRBD-operations" \
   op monitor interval="20" role="Master" timeout="20" \
   op monitor interval="30" role="Slave" timeout="20"
ms msDRBD resDRBD \
   meta notify="true" master-max="2" interleave="true"
primitive resDLM ocf:pacemaker:controld op monitor interval="120s"
   clone cloneDLM resDLM meta globally-unique="false" interleave="true"
colocation colDLMDRBD inf: cloneDLM msDRBD:Master
order ordDRBDDLM 0: msDRBD:promote cloneDLM
primitive resO2CB ocf:pacemaker:o2cb op monitor interval="120s"
clone cloneO2CB resO2CB meta globally-unique="false" interleave="true"
colocation colO2CBDLM inf: cloneO2CB cloneDLM
order ordDLMO2CB 0: cloneDLM cloneO2CB
primitive resFS ocf:heartbeat:Filesystem \
   params device="/dev/drbd/by-res/vms" directory="/var/lib/libvirt/images/ocfs" fstype="ocfs2" \
   op monitor interval="120s"
clone cloneFS resFS meta interleave="true" ordered="true"
colocation colFSO2CB inf: cloneFS cloneO2CB
order ordO2CBFS 0: cloneO2CB cloneFS
#apt-get install dlm #nao precisa
#### ERRO

### TESTE COLOCATION ORDER COM SET
colocation COL_DRBD_OCFS inf: OCFS_MOUNT_CLONE MS_DRBD:Master
colocation COL_VM1 inf: VM1 OCFS_MOUNT_CLONE
colocation COL_VM2 inf: VM2 OCFS_MOUNT_CLONE
order ORD_DRBD_OCFS inf: MS_DRBD:promote OCFS_MOUNT_CLONE:start
order ORD_VM1 inf: OCFS_MOUNT_CLONE:start VM1:start
order ORD_VM2 inf: OCFS_MOUNT_CLONE:start VM2:start
# TESTAR:
colocation COL_DRBD_OCFS inf: ( VM1 VM2 ) OCFS_MOUNT_CLONE MS_DRBD:Master
order ORD_DRBD_OCFS inf: MS_DRBD:promote OCFS_MOUNT_CLONE:start ( VM1 VM2 )
#cib:
#C->D ->E
#     ->F
<resource_set id="ordered-set-2" sequential="true">
 <resource_ref id="C"/>
 <resource_ref id="D"/>
</resource_set>
<resource_set id="ordered-set-3" sequential="false">
 <resource_ref id="E"/>
 <resource_ref id="F"/>
</resource_set>
# NAO FUNCIONOU, NAO INICIOU O DRBD


#tentativa 2
crm configure edit
node $id="1084752392" brunonote.emer.com
node $id="1084752393" bruno2.emer.com
# drbd do resource vms
primitive DRBD ocf:linbit:drbd \
	params drbd_resource="vms" \
	op monitor interval="20" role="Master" timeout="240" \
	op monitor interval="30" role="Slave" timeout="240" \
	meta is-managed="true" target-role="Started"
# monta filesystem
primitive OCFS_MOUNT ocf:heartbeat:Filesystem \
	params device="/dev/drbd/by-res/vms" directory="/var/lib/libvirt/images/ocfs" fstype="ocfs2" \
	meta target-role="Started"
# master/master do drbd
ms MS_DRBD DRBD \
	meta master-max="2" clone-max="2" notify="true" interleave="true" allow-migrate="true" is-managed="true"
# replica configuracao do filesystem OCFS_MOUNT
clone OCFS_MOUNT_CLONE OCFS_MOUNT \
	meta interleave="true" ordered="true"
# dependencias dos recursos, OCFS_MOUNT_CLONE depende de MS_DRBD, para rodar no mesmo node
colocation COL_DRBD_OCFS inf: OCFS_MOUNT_CLONE MS_DRBD:Master
# vm com live migration
primitive VM1 ocf:heartbeat:VirtualDomain \
        params config="/etc/libvirt/qemu/vm1.xml" hypervisor="qemu:///system" migration_transport="ssh" force_stop="0" \
        op monitor timeout="30" interval="10" depth="0" \
        op start timeout="90" interval="0" \
        op stop timeout="90" interval="0" \
        op migrate_from interval="0" timeout="240" \
        op migrate_to interval="0" timeout="240" \
        meta allow-migrate="true" target-role="Started"
# dependencias dos recursos, VM1 depende de MS_DRBD, para rodar no mesmo node
colocation COL_VMS inf: VM1 MS_DRBD:Master
# ordena inicializacao de servicos, primeiro drbd, filesystem
order ORD inf: MS_DRBD:promote OCFS_MOUNT_CLONE:start
# para preferencia do node que vm vai iniciar
location cli-prefer-VM1 VM1 inf: brunonote.emer.com
# no secundario caso primario falhe ou fique em standby
location backup-VM1 VM1 100: brunonote.emer.com
#outros
property $id="cib-bootstrap-options" \
	dc-version="1.1.10-42f2063" \
	cluster-infrastructure="corosync" \
# para recurso nao ser movido entre os nodes
	default-resource-stickiness="1000" \
	no-quorum-policy="ignore" \
	stonith-enabled="false" \
	last-lrm-refresh="1471491727"

# CONFIG final com live migration e fail over automatico
node $id="1084752392" brunonote.emer.com \
        attributes maintenance="off" standby="off"
node $id="1084752393" bruno2.emer.com \
        attributes standby="on"
primitive DRBD ocf:linbit:drbd \
        params drbd_resource="vms" \
        op monitor interval="20" role="Master" timeout="240" \
        op monitor interval="30" role="Slave" timeout="240" \
        meta is-managed="true" target-role="Started"
primitive OCFS_MOUNT ocf:heartbeat:Filesystem \
        params device="/dev/drbd/by-res/vms" directory="/var/lib/libvirt/images/ocfs" fstype="ocfs2"
primitive VM1 ocf:heartbeat:VirtualDomain \
        params config="/etc/libvirt/qemu/vm1.xml" hypervisor="qemu:///system" migration_transport="ssh" force_stop="0" \
        op monitor timeout="30" interval="10" depth="0" \
        op start timeout="90" interval="0" \
        op stop timeout="90" interval="0" \
        op migrate_from interval="0" timeout="240" \
        op migrate_to interval="0" timeout="240" \
        meta allow-migrate="true" target-role="Started" is-managed="true" migration-threshold="5" \
        utilization cpu="1" hv_memory="512"
primitive VM2 ocf:heartbeat:VirtualDomain \
        params config="/etc/libvirt/qemu/vm2.xml" hypervisor="qemu:///system" migration_transport="ssh" force_stop="0" \
        op monitor timeout="30" interval="10" depth="0" \
        op start timeout="90" interval="0" \
        op stop timeout="90" interval="0" \
        op migrate_from interval="0" timeout="240" \
        op migrate_to interval="0" timeout="240" \
        meta allow-migrate="true" target-role="Started" is-managed="true" migration-threshold="5" \
        utilization cpu="1" hv_memory="512"
ms MS_DRBD DRBD \
        meta master-max="2" clone-max="2" notify="true" interleave="true" allow-migrate="true" is-managed="true"
clone OCFS_MOUNT_CLONE OCFS_MOUNT \
        meta interleave="true" ordered="true" target-role="Started"
location backup-VM1 VM1 100: brunonote.emer.com
location backup-VM2 VM2 100: bruno2.emer.com
location cli-prefer-VM1 VM1 inf: bruno2.emer.com
location cli-prefer-VM2 VM2 inf: brunonote.emer.com
colocation COL_DRBD_OCFS inf: OCFS_MOUNT_CLONE MS_DRBD:Master
colocation COL_VM1 inf: VM1 OCFS_MOUNT_CLONE
colocation COL_VM2 inf: VM2 OCFS_MOUNT_CLONE
order ORD_DRBD_OCFS inf: MS_DRBD:promote OCFS_MOUNT_CLONE:start
order ORD_VM1 inf: OCFS_MOUNT_CLONE:start VM1:start
order ORD_VM2 inf: OCFS_MOUNT_CLONE:start VM2:start
property $id="cib-bootstrap-options" \
        dc-version="1.1.10-42f2063" \
        cluster-infrastructure="corosync" \
        default-resource-stickiness="1000" \
        no-quorum-policy="ignore" \
        stonith-enabled="false" \
        last-lrm-refresh="1471783143"

#VM
#pool
virsh pool-create-as ocfs --type=dir --target=/var/lib/libvirt/images/ocfs
#xml vm
<domain type='kvm'>
  <name>vm1</name>
  <uuid>eca754c1-099d-4129-9268-43c114e11c8a</uuid>
  <memory unit='KiB'>524288</memory>
  <currentMemory unit='KiB'>524288</currentMemory>
  <vcpu placement='static'>1</vcpu>
  <os>
    <type arch='x86_64' machine='pc-i440fx-trusty'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/bin/kvm-spice</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' />
      <source file='/var/lib/libvirt/images/ocfs/vm1.img'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
    <controller type='usb' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x2'/>
    </controller>
    <controller type='pci' index='0' model='pci-root'/>
    <interface type='network'>
      <mac address='52:54:00:3b:df:98'/>
      <source bridge='br0'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <input type='keyboard' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <sound model='ich6'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </memballoon>
  </devices>
</domain>

scp /home/bruno/vm1.xml bruno2:/home/bruno/
ssh bruno2 virsh define /home/bruno/vm1.xml


#ver problema se ocorrer novamente de um no offline ???
#tentar com tag nodes, #parece que funcinou
#ao iniciar node 2 aparece offline um para o outro e da splitbrain ??
#esta subindo sem quorum, se usar no-quorum-policy=stop, quando um node cair derruba todos os recursos de todos nodes
#TESTAR SUBIR UM NO ESPERAR E LIGAR O OUTRO (NAO LIGAR SIMULTANEAMENTE) nao funcionou, os dois subiram os recursos juntos
#http://serverfault.com/questions/578141/2-corosync-nodes-not-online-together-one-offline-one-online
#sem mcastaddr 
totem {
	...
        interface {
                # The following values need to be set based on your environment 
                member {
                        memberaddr: 192.168.2.8
                }
                member {
                        memberaddr: 192.168.2.9
                }
                ringnumber: 0
                bindnetaddr: 192.168.2.0
        }
        transport: udpu
}
#FUNCIONOU
#se der problema ainda testar:
no-quorum-policy:
• ignore: continue all resource management
• freeze: continue resource management, but don’t
recover resources from nodes not in the affected
partition
• stop: stop all resources in the affected cluster
partition
• suicide: fence all nodes in the affected cluster
partition


# web interface gui
https://github.com/ClusterLabs/hawk
https://hawk-guide.readthedocs.io/en/latest/

#suse ou redhat
git clone https://github.com/ClusterLabs/hawk.git
vi Makefile
WWW_BASE = /var/www
INIT_STYLE = redhat ???????

apt-get install libpam0g-dev

#debian
http://oss.clusterlabs.org/pipermail/pacemaker/2011-August/011263.html
https://github.com/rndsolutions/hawkcd/wiki/Setup-Hawk-Server-on-Linux-(Ubuntu)-Environment
??
AGUARDAR


### PRODUCAO implementar Resource-Level Fencing (isolamento nivel de servico) ja implementado no drbd (fencing resource-only)
#precisa configurar 2 rings de comunicacao, um pelo swtich e outro direto
# ou implementar Node-Level Fencing, mas precisa de uma interface separada iDRAC da dell ou placa IPMI por exemplo
# Resource-Level Fencing
#na pesquisa linux_magazine_pacemaker_isolamento.jpg
#https://www.sebastien-han.fr/blog/2012/08/01/corosync-rrp-configuration/
#corosync.conf
totem {
    version: 2
    secauth: on
    threads: 0
    rrp_mode: passive
    interface {
        ringnumber: 0
        bindnetaddr: 10.0.0.0
        mcastaddr: 226.94.1.1
        mcastport: 5405
        ttl: 1
        }
    interface {
        ringnumber: 1
        bindnetaddr: 172.16.0.0
        mcastaddr: 226.94.1.2
        mcastport: 5407
        ttl: 1
    }
}
# e drbd como fazer ring redundante?
# nao da com drbd

#STONITH para producao
#na pesquisa linux_magazine_pacemaker_isolamento.jpg
http://www.linuxjournal.com/content/ahead-pack-pacemaker-high-availability-stack?page=0,1
apt-get install ipmitool
echo "ipmi_devintf" >> /etc/modules
echo "ipmi_si" >> /etc/modules
#teste ipmi
/usr/bin/ipmitool -I lan -U root -P genese -H 172.31.0.240 chassis power status
stonith -t external/ipmi ipaddr=172.31.0.240 userid=root password=somepass -S
#testar pacemaker por ipmi
primitive p_ipmi_alice stonith:external/ipmi \
  params hostname="alice" ipaddr="192.168.15.1" \
    userid="admin" passwd="foobar" \
  op start interval="0" timeout="60" \
  op monitor interval="120" timeout="60"
primitive p_ipmi_bob stonith:external/ipmi \
  params hostname="bob" ipaddr="192.168.15.2" \
    userid="admin" passwd="foobar" \
  op start interval="0" timeout="60" \
  op monitor interval="120" timeout="60"
primitive p_ipmi_charlie stonith:external/ipmi \
  params hostname="charlie" ipaddr="192.168.15.3" \
    userid="admin" passwd="foobar" \
  op start interval="0" timeout="60" \
  op monitor interval="120" timeout="60"
location l_ipmi_alice p_ipmi_alice -inf: alice
location l_ipmi_bob p_ipmi_bob -inf: bob
location l_ipmi_charlie p_ipmi_charlie -inf: charlie
property stonith-enabled="true"
#
https://hawk-guide.readthedocs.io/en/latest/stonith.html
https://www.drbd.org/en/doc/users-guide-84/ch-ocfs2
http://clusterlabs.org/wiki/Guest_Fencing #redhat um node com todas vms
#drbd resource:
disk {
        fencing resource-and-stonith;
}
handlers {
        # Make sure the other node is confirmed
        # dead after this!
        fence-peer "/sbin/kill-other-node.sh";
}
#devices para utilizar:
stonith_admin --list-installed
#detalhes do device stonith
stonith_admin --metadata --agent external/ipmi
crm configure ra info stonith:external/ipmi


#ver para quando OCFS_MOUNT nao iniciar, fazer retentativa ou monitorar
# OK

#ver como promover no DC para outro node manualmente

#resincrinizar primario
drbdadm -- --overwrite-data-of-peer primary vms
#resincronizar secundary
drbdadm -- --discard-my-data connect vms


### RECURAR FALHA DE REDE
# problema drbd split brain
primario
m:res  cs          ro               ds                 p       mounted  fstype
0:vms  StandAlone  Primary/Unknown  UpToDate/Outdated  r-----  ocfs2
secundario
m:res  cs            ro                 ds                 p  mounted  fstype
0:vms  WFConnection  Secondary/Unknown  UpToDate/Outdated  C

https://www.hastexo.com/resources/hints-and-kinks/solve-drbd-split-brain-4-steps/
https://www.drbd.org/en/doc/users-guide-83/s-resolve-split-brain

##tentativa 1
#no principal
#desligar servicos
crm node standby piova.redesul.com.br
#alterar para secundario
drbdadm up vms
drbdadm secondary vms
#no vitima, tera dados sobrepostos
drbdadm up vms
drbdadm secondary vms
drbdadm disconnect vms
drbdadm invalidate vms
drbdadm connect --discard-my-data vms
#vai resincronizar
#erro no live migration as vezes apos fazer isso

##tentativa 2
##testar sem derrubar no primario
#no vitima
crm node maintenance piova.redesul.com.br
crm node maintenance brina.redesul.com.br
umount /dev/drbd/by-res/vms
drbdadm secondary vms
drbdadm invalidate vms
drbdadm connect --discard-my-data vms

#no principal
drbdadm connect vms
#aguardar sincronizar

crm node ready piova.redesul.com.br
crm node ready brina.redesul.com.br
#testar ??? PARECE QUE RESOLVEU, TESTAR EM PRODUCAO 
#DEU PROBLEMA

##tentativa 3
#no principal
#desligar servicos
crm node standby piova.redesul.com.br
#alterar para secundario
drbdadm up vms
drbdadm secondary vms
#no vitima, tera dados sobrepostos
drbdadm up vms
drbdadm secondary vms
drbdadm disconnect vms
drbdadm invalidate vms
drbdadm connect --discard-my-data vms
#vai resincronizar
crm node online piova.redesul.com.br
#nao funcionou

#tentativa 4 - PROCEDIMENTO PARA RECURAR FALHA DE REDE
#no principal
#desligar servicos
crm node maintenance piova.redesul.com.br
crm node maintenance brina.redesul.com.br
#no vitima, tera dados sobrepostos
drbdadm up vms
drbdadm secondary vms
drbdadm disconnect vms
drbdadm connect --discard-my-data vms
#no principal
drbdadm connect vms
#ver resync e reiniciar vms
crm node ready piova.redesul.com.br
crm node standby piova.redesul.com.br
crm node standby brina.redesul.com.br
crm node ready brina.redesul.com.br
crm node online brina.redesul.com.br
crm node online piova.redesul.com.br
#FUNCIONOU


#testar configuracao corosync com two nodes no quorum
quorum {
   provider: corosync_votequorum
   expected_votes: 2
   two_node: 1
}


#testar falhas reiniciando vm para ver se migra de node
#ver migration-threshold, numero de falhas para migrar o recurso para outro node
# FUNCIONOU OK


