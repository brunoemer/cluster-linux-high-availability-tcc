brina 186.195.16.14
piova 186.195.16.6

# OS


# rede
apt-get install bridge-utils

# bond - link aggregation
apt-get install ifenslave-2.6
sh -c 'grep -q bonding /etc/modules || echo bonding >> /etc/modules'

# vlan
apt-get install vlan
sudo  sh -c 'grep -q 8021q /etc/modules || echo 8021q >> /etc/modules'
sudo modprobe 8021q

echo 'net.bridge.bridge-nf-call-ip6tables = 0' >> /etc/sysctl.conf
echo 'net.bridge.bridge-nf-call-iptables = 0' >> /etc/sysctl.conf
echo 'net.bridge.bridge-nf-call-arptables = 0' >> /etc/sysctl.conf
echo 'net.bridge.bridge-nf-filter-pppoe-tagged = 0' >> /etc/sysctl.conf
echo 'net.bridge.bridge-nf-filter-vlan-tagged = 0' >> /etc/sysctl.conf


# editar /etc/network/interfaces
auto eth0
iface eth0 inet manual
bond-master bond0

auto eth1
iface eth1 inet manual
bond-master bond0

auto bond0
iface bond0 inet manual
       bond-mode 4
       bond-slaves eth0 eth1
       bond-lacp-rate 1
       bond-miimon 100
       bond-xmit_hash_policy layer3+4
#       bond-ad-select 1

# The primary network interface
auto br0
iface br0 inet static
	address 186.195.16.x
	netmask 255.255.255.0
	network 186.195.16.0
	broadcast 186.195.16.255
	gateway 186.195.16.254
	# dns-* options are implemented by the resolvconf package, if installed
	dns-nameservers 186.195.16.1 186.195.16.2 186.195.16.3
	dns-search redesul.com.br

        bridge_ports bond0
        bridge_stp off
        bridge_maxwait 0


# configuracao discos lvm
lvcreate -n lvdrbd vg0 -L 500G

# drbd
apt-get install drbd8-utils drbdlinks

# editar /etc/drbd.d/global_common.conf
global {
        usage-count yes;
        minor-count 16;
}
# editar /etc/drbd.d/vms.res
resource vms {
    meta-disk internal;
    device /dev/drbd0;
    protocol C;
    disk {
        fencing resource-only;
    }
    handlers {
        fence-peer "/usr/lib/drbd/crm-fence-peer.sh";
        after-resync-target "/usr/lib/drbd/crm-unfence-peer.sh";
    }
    net {
        allow-two-primaries;
    }
    startup {
        become-primary-on both;
    }
    on brina.redesul.com.br {
        address 186.195.16.14:7791;
        disk /dev/vg0/lvdrbd;
    }
    on piova.redesul.com.br {
        address 186.195.16.6:7791;
        disk /dev/vg0/lvdrbd;
    }
    syncer {
        rate 50M;
    }
}

#brina
ufw allow from 186.195.16.6
echo "brina.redesul.com.br" > /etc/hostname
#piova
ufw allow from 186.195.16.14
echo "piova.redesul.com.br" > /etc/hostname

service drbd restart

drbdadm create-md vms
drbdadm up vms
# em um node
drbdadm -- --overwrite-data-of-peer primary vms
# aguardar sincronizar
# outro no
drbdadm primary vms

service drbd status

# ocfs
apt-get install ocfs2-tools

#editar /etc/ocfs2/cluster.conf
node:
        ip_port = 7777
        ip_address = 186.195.16.14
        number = 0
        name = brina.redesul.com.br
        cluster = clusterocfs
node:
        ip_port = 7777
        ip_address = 186.195.16.6
        number = 1
        name = piova.redesul.com.br
        cluster = clusterocfs
cluster:
        node_count = 2
        name = clusterocfs

#editar /etc/default/o2cb
# O2CB_ENABLED: 'true' means to load the driver on boot.
O2CB_ENABLED=true

# O2CB_BOOTCLUSTER: If not empty, the name of a cluster to start.
O2CB_BOOTCLUSTER=clusterocfs

# O2CB_HEARTBEAT_THRESHOLD: Iterations before a node is considered dead.
O2CB_HEARTBEAT_THRESHOLD=31

# O2CB_IDLE_TIMEOUT_MS: Time in ms before a network connection is considered dead.
O2CB_IDLE_TIMEOUT_MS=30000

# O2CB_KEEPALIVE_DELAY_MS: Max. time in ms before a keepalive packet is sent.
O2CB_KEEPALIVE_DELAY_MS=2000

# O2CB_RECONNECT_DELAY_MS: Min. time in ms between connection attempts.
O2CB_RECONNECT_DELAY_MS=2000

service ocfs2 restart

mkfs.ocfs2 /dev/drbd/by-res/vms

# verificar se esta executando
service o2cb status

# teste ocfs
mkdir /var/lib/libvirt/images/ocfs
mount -t ocfs2 /dev/drbd/by-res/vms /var/lib/libvirt/images/ocfs/

# bug ocfs2 is not compatible with your environment.
# fix bug https://bugs.launchpad.net/ubuntu/+source/ocfs2-tools/+bug/1412438
#patch /usr/lib/ocf/resource.d/heartbeat/Filesystem
--- Filesystem 2013-12-16 07:41:25.000000000 +0000
+++ Filesystem.new 2015-01-19 19:01:30.181772112 +0000
@@ -338,7 +338,7 @@ ocfs2_init()
  # not need this:
  OCFS2_SLES10=""
- if [ "X$HA_cluster_type" = "Xcman" ]; then
+ if [ "X$HA_cluster_type" = "Xcorosync" ]; then
      return
  elif [ "X$HA_cluster_type" != "Xopenais" ]; then
   if grep -q "SUSE Linux Enterprise Server 10" /etc/SuSE-release >/dev/null 2>&1 ; then


# virtualizacao kvm
apt-get install -y qemu-kvm libvirt-bin

# criar pool
virsh pool-create-as ocfs --type=dir --target=/var/lib/libvirt/images/ocfs


# cluster pacemaker corosync
apt-get install pacemaker corosync

# editar /etc/corosync/corosync.conf
totem {
	version: 2
	token: 3000
	token_retransmits_before_loss_const: 10
	join: 60
	consensus: 3600
	vsftype: none
	max_messages: 20
	clear_node_high_bit: yes
 	secauth: on
 	threads: 0
 	rrp_mode: none
        interface {
                # The following values need to be set based on your environment 
                member {
                        memberaddr: 186.195.16.14
                }
                member {
                        memberaddr: 186.195.16.6
                }
                ringnumber: 0
                bindnetaddr: 186.195.16.0
        }
        transport: udpu
}
amf {
	mode: disabled
}
quorum {
        # Quorum for the Pacemaker Cluster Resource Manager
        provider: corosync_votequorum
        expected_votes: 2
}
service {
        # Load the Pacemaker Cluster Resource Manager
        ver:       0
        name:      pacemaker
}
aisexec {
        user:   root
        group:  root
}
logging {
        fileline: off
        to_stderr: yes
        to_logfile: no
        to_syslog: yes
        syslog_facility: daemon
        debug: off
        timestamp: on
        logger_subsys {
                subsys: AMF
                debug: off
                tags: enter|leave|trace1|trace2|trace3|trace4|trace6
        }
}

# node brina
corosync-keygen
scp /etc/corosync/authkey bruno@piova.redesul.com.br:
# node piova
mv /home/bruno/authkey /etc/corosync/authkey
chown root:root /etc/corosync/authkey
chmod 400 /etc/corosync/authkey

# editar /etc/default/corosync
START=yes

service corosync restart
service pacemaker restart

# teste pacemaker
root@brina:~# crm status
Last updated: Thu Aug 25 15:03:20 2016
Last change: Thu Aug 25 15:03:10 2016 via crmd on piova.redesul.com.br
Stack: corosync
Current DC: piova.redesul.com.br (985862150) - partition with quorum
Version: 1.1.10-42f2063
2 Nodes configured
0 Resources configured

Online: [ brina.redesul.com.br piova.redesul.com.br ]


# configuracao pacemaker
crm configure edit

node $id="985862150" piova.redesul.com.br
node $id="985862158" brina.redesul.com.br

primitive DRBD ocf:linbit:drbd \
        params drbd_resource="vms" \
        op monitor interval="20" role="Master" timeout="240" \
        op monitor interval="30" role="Slave" timeout="240" \
        meta is-managed="true" target-role="Started"
primitive OCFS_MOUNT ocf:heartbeat:Filesystem \
        params device="/dev/drbd/by-res/vms" directory="/var/lib/libvirt/images/ocfs" fstype="ocfs2"
ms MS_DRBD DRBD \
        meta master-max="2" clone-max="2" notify="true" interleave="true" allow-migrate="true" is-managed="true"
clone OCFS_MOUNT_CLONE OCFS_MOUNT \
        meta interleave="true" ordered="true" target-role="Started"
colocation COL_DRBD_OCFS inf: OCFS_MOUNT_CLONE MS_DRBD:Master
order ORD_DRBD_OCFS inf: MS_DRBD:promote OCFS_MOUNT_CLONE:start

property $id="cib-bootstrap-options" \
	dc-version="1.1.10-42f2063" \
	cluster-infrastructure="corosync" \
        default-resource-stickiness="1000" \
        no-quorum-policy="ignore" \
        stonith-enabled="false"


#### PAREI AQUI #####


# VMS
primitive VM1 ocf:heartbeat:VirtualDomain \
        params config="/etc/libvirt/qemu/vm1.xml" hypervisor="qemu:///system" migration_transport="ssh" force_stop="0" \
        op monitor timeout="30" interval="10" depth="0" \
        op start timeout="90" interval="0" \
        op stop timeout="90" interval="0" \
        op migrate_from interval="0" timeout="240" \
        op migrate_to interval="0" timeout="240" \
        meta allow-migrate="true" target-role="Started" is-managed="true" \
        utilization cpu="1" hv_memory="512"
primitive VM2 ocf:heartbeat:VirtualDomain \
        params config="/etc/libvirt/qemu/vm2.xml" hypervisor="qemu:///system" migration_transport="ssh" force_stop="0" \
        op monitor timeout="30" interval="10" depth="0" \
        op start timeout="90" interval="0" \
        op stop timeout="90" interval="0" \
        op migrate_from interval="0" timeout="240" \
        op migrate_to interval="0" timeout="240" \
        meta allow-migrate="true" target-role="Started" \
        utilization cpu="1" hv_memory="512"
location backup-VM1 VM1 100: brunonote.emer.com
location backup-VM2 VM2 100: bruno2.emer.com
location cli-prefer-VM1 VM1 inf: bruno2.emer.com
location cli-prefer-VM2 VM2 inf: brunonote.emer.com
colocation COL_VM1 inf: VM1 OCFS_MOUNT_CLONE
colocation COL_VM2 inf: VM2 OCFS_MOUNT_CLONE
order ORD_VM1 inf: OCFS_MOUNT_CLONE:start VM1:start
order ORD_VM2 inf: OCFS_MOUNT_CLONE:start VM2:start


